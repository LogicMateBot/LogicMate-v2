{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo (You Only Look Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mroboflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Roboflow, Project\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Literal, Pattern\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import uuid\n",
    "import logging\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from roboflow import Roboflow, Project\n",
    "from typing import Any, Literal, Pattern\n",
    "from roboflow.core.dataset import Dataset\n",
    "from roboflow.core.version import Version\n",
    "from supervision.detection.core import Detections\n",
    "from torch.cuda import is_available, empty_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class DirectoryUtil:\n",
    "    \"\"\"Utility class for handling directories.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def directory_exists(path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the specified directory exists.\n",
    "        \n",
    "        Args:\n",
    "            path (str): The directory path.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if the directory exists, False otherwise.\n",
    "        \"\"\"\n",
    "        return os.path.isdir(s=path)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_directory(path: str) -> None:\n",
    "        \"\"\"\n",
    "        Creates a directory at the specified path.\n",
    "        If the directory already exists, no exception is raised.\n",
    "        \n",
    "        Args:\n",
    "            path (str): The directory path.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(name=path, exist_ok=True)\n",
    "            logging.info(msg=f\"Directory created: {path}\")\n",
    "        except Exception as e:\n",
    "            logging.info(msg=f\"Error creating directory {path}: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def ensure_directory(path: str) -> str:\n",
    "        \"\"\"\n",
    "        Ensures that the directory exists. If it does not, the directory is created.\n",
    "        \n",
    "        Args:\n",
    "            path (str): The directory path.\n",
    "\n",
    "        Return:\n",
    "            str: Directory path.\n",
    "        \"\"\"\n",
    "        if not DirectoryUtil.directory_exists(path):\n",
    "            DirectoryUtil.create_directory(path)\n",
    "            return path\n",
    "        else:\n",
    "            logging.info(msg=f\"Directory already exists: {path}\")\n",
    "            return path\n",
    "\n",
    "    @staticmethod\n",
    "    def find_downloaded_dataset(datasets_path: str, project_id: str, version_number: int) -> str:\n",
    "        # logging.info(f\"Directories in '{datasets_path}': {os.listdir(datasets_path)}\")\n",
    "\n",
    "        expected_name = f\"{project_id}-{version_number}\"\n",
    "        pattern: Pattern[str] = re.compile(re.escape(pattern=expected_name), re.IGNORECASE)\n",
    "\n",
    "        for folder in os.listdir(path=datasets_path):\n",
    "            folder_path: str = os.path.join(datasets_path, folder)\n",
    "\n",
    "            # logging.info(f\"Comparing: '{folder}' vs '{expected_name}'\")\n",
    "\n",
    "            if os.path.isdir(s=folder_path) and pattern.search(string=folder):\n",
    "                return folder_path \n",
    "\n",
    "        logging.error(f\"Dataset not found for: '{project_id}-{version_number}' in '{datasets_path}'.\")\n",
    "        raise FileNotFoundError(f\"Dataset not found for: '{project_id}-{version_number}' in '{datasets_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'YOLO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# trained_model = model.train(\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#   data=CODE_DATASET_YAML_FILE,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#   epochs=300,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#   copy_paste=0.5  # Mejor detección de objetos difíciles\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mModel\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: YOLO, data: \u001b[38;5;28mstr\u001b[39m, epochs: \u001b[38;5;28mint\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m, batch: \u001b[38;5;28mint\u001b[39m, imgsz: \u001b[38;5;28mint\u001b[39m, optimizer: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n",
      "Cell \u001b[0;32mIn[24], line 20\u001b[0m, in \u001b[0;36mModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mModel\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: \u001b[43mYOLO\u001b[49m, data: \u001b[38;5;28mstr\u001b[39m, epochs: \u001b[38;5;28mint\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m, batch: \u001b[38;5;28mint\u001b[39m, imgsz: \u001b[38;5;28mint\u001b[39m, optimizer: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'YOLO' is not defined"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# trained_model = model.train(\n",
    "#   data=CODE_DATASET_YAML_FILE,\n",
    "#   epochs=300,\n",
    "#   device='cuda',\n",
    "#   batch=14,\n",
    "#   patience=75,\n",
    "#   optimizer=\"AdamW\",\n",
    "#   lr0=0.0005,  # Tasa de aprendizaje más baja para evitar falsos positivos\n",
    "#   weight_decay=0.0005,  # Regularización para evitar sobreajuste\n",
    "#   momentum=0.9,\n",
    "#   warmup_epochs=5,\n",
    "#   mixup=0.3,  # Más diversidad en el dataset\n",
    "#   mosaic=0.7,  # Menos distorsión en imágenes\n",
    "#   copy_paste=0.5  # Mejor detección de objetos difíciles\n",
    "# )\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, model: YOLO, data: str, epochs: int, device: str, batch: int, imgsz: int, optimizer: str) -> None:\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.batch = batch\n",
    "        self.imgsz = imgsz\n",
    "        self.optimizer = optimizer\n",
    "        self.lr0 = 0.001\n",
    "        self.patience = None\n",
    "        self.weight_decay = None\n",
    "        self.momentum = None\n",
    "        self.warmup_epochs = None\n",
    "        self.mixup = None\n",
    "        self.mosaic = None\n",
    "        self.copy_paste = None\n",
    "        \n",
    "    def train(self) -> None:\n",
    "        all_train_params: dict[str, str | int | float | None] = { \n",
    "            \"data\": self.data,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"device\": self.device,\n",
    "            \"batch\": self.batch,\n",
    "            \"patience\": self.patience,\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr0\": self.lr0,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"momentum\": self.momentum,\n",
    "            \"warmup_epochs\": self.warmup_epochs,\n",
    "            \"mixup\": self.mixup,\n",
    "            \"mosaic\": self.mosaic,\n",
    "            \"copy_paste\": self.copy_paste,\n",
    "        }\n",
    "\n",
    "        train_params: dict[str, str | int | float]  = {key: value for key, value in all_train_params.items() if value is not None}\n",
    "\n",
    "        logging.info(msg=f\"Training model with parameters: {train_params}\")\n",
    "        # self.model.train(**train_params)\n",
    "\n",
    "\n",
    "class RoboflowDataset:\n",
    "    def __init__(self, \n",
    "                datasets_path: str, \n",
    "                api_key: str, \n",
    "                the_workspace: str, \n",
    "                project_id: str, \n",
    "                version_number: int,\n",
    "                model_format: str) -> None:\n",
    "\n",
    "        self.datasets_path = datasets_path\n",
    "        self.api_key = api_key\n",
    "        self.the_workspace = the_workspace\n",
    "        self.project_id = project_id\n",
    "        self.version_number = version_number\n",
    "        self.model_format = model_format\n",
    "        self.dataset_path = None\n",
    "\n",
    "\n",
    "    def download_dataset(self) -> None:\n",
    "        try:\n",
    "            os.chdir(path=self.datasets_path)\n",
    "            rf: Roboflow = Roboflow(api_key=self.api_key)\n",
    "            project: Project = rf.workspace(the_workspace=self.the_workspace).project(project_id=self.project_id)\n",
    "            version: Version = project.version(version_number=self.version_number)\n",
    "            dataset: Dataset = version.download(model_format=self.model_format)\n",
    "        except:\n",
    "            logging.error(msg=f\"Something went wrong downloading the dataset\")\n",
    "            ValueError('Something went wrong downloading the dataset.')\n",
    "    \n",
    "    def find_dataset_path(self) -> str | None:\n",
    "        try:\n",
    "            dataset_path: str = DirectoryUtil.find_downloaded_dataset(datasets_path=self.datasets_path,\n",
    "                                                                    project_id=self.project_id,\n",
    "                                                                    version_number=self.version_number)\n",
    "            logging.info(msg=f\"Dataset is: {dataset_path}\")\n",
    "            self.dataset_path = dataset_path\n",
    "            return dataset_path\n",
    "        except:\n",
    "            logging.error(msg=f\"Something went wrong finding the dataset\")\n",
    "            ValueError('Something went wrong finding the dataset.')\n",
    "    \n",
    "    def prepare_dataset_datayml(self) -> None:\n",
    "        if self.dataset_path is None:\n",
    "            logging.error(msg=f\"Dataset path is not set. Run find_dataset_path() first.\")\n",
    "            raise ValueError(\"Dataset path is not set. Run find_dataset_path() first.\")\n",
    "\n",
    "        dataset_datayml_path: str = os.path.join(self.dataset_path, 'data.yaml')\n",
    "        # logging.info(msg=f\"dataset_datayml_path {dataset_datayml_path}\")\n",
    "\n",
    "        with open(file=dataset_datayml_path, mode='r') as file:\n",
    "            datayml_file: list[str] = file.readlines()\n",
    "\n",
    "        # logging.info(msg=f\"datayml_file {datayml_file} \")\n",
    "\n",
    "        pattern: Pattern[str] = re.compile(r\"https?://[^\\s]+\") \n",
    "\n",
    "        if(any(pattern.search(line) for line in datayml_file)):\n",
    "\n",
    "            logging.info(msg=f\"Dataset is not prepared. Preparing it...\")\n",
    "            datayml_file = datayml_file[:-4]\n",
    "            logging.info(msg=f\"Removed unneccesary content.\")\n",
    "\n",
    "            datayml_file = [line.replace(\"../train/images\", f\"{self.dataset_path}/train/images\") for line in datayml_file]\n",
    "            datayml_file = [line.replace(\"../valid/images\", f\"{self.dataset_path}/valid/images\") for line in datayml_file]\n",
    "            datayml_file = [line.replace(\"../test/images\", f\"{self.dataset_path}/test/images\") for line in datayml_file]\n",
    "\n",
    "            logging.info(msg=f\"Added correct train, valid and test images path.\")\n",
    "\n",
    "            with open(file=dataset_datayml_path,mode= 'w') as f:\n",
    "                f.writelines(datayml_file)\n",
    "\n",
    "            logging.info(msg=f\"Saved prepared file.\")\n",
    "        \n",
    "        logging.info(msg=f\"Dataset is ready to use for training.\")\n",
    "\n",
    "class Yolo:\n",
    "    def __init__(self,\n",
    "                models_dir: str,\n",
    "                yolo_model: str = \"yolov12l.pt\",\n",
    "                min_code_prediction_threshold: int = 3,\n",
    "                min_diagram_prediction_threshold: int = 3) -> None:\n",
    "\n",
    "        self.models_dir = models_dir\n",
    "        self.yolo_model = yolo_model\n",
    "\n",
    "        yolo_path: str = os.path.join(models_dir, \"yolov12\")\n",
    "        if os.path.isdir(s=yolo_path):\n",
    "            logging.info(msg=\"Yolo was found. Initializing.\")\n",
    "        else:\n",
    "            logging.info(msg=\"Yolo was not found, dowloading it...\")\n",
    "            os.chdir(path=self.models_dir)\n",
    "            repo_url: str = f\"https://github.com/sunsmarterjie/yolov12\"\n",
    "\n",
    "            try:\n",
    "                logging.info(msg=\"Downloading...\")\n",
    "                subprocess.run([\"git\", \"clone\", repo_url, yolo_path], check=True)\n",
    "                logging.info(msg=\"Downloaded completed...\")\n",
    "            except subprocess.CalledProcessError:\n",
    "                logging.error(msg=\"Cloud not download yolov12 from git. Check if git is installed.\")\n",
    "                RuntimeError(\"Cloud not download yolov12 from git. Check if git is installed.\")\n",
    "            \n",
    "            commands: list[list[str]] = [\n",
    "                [\"pip\", \"install\", \"roboflow\", \"supervision\", \"flash-attn\", \"--upgrade\", \"-q\"],\n",
    "                [\"pip\", \"install\", \"-r\", \"requirements.txt\"],\n",
    "                [\"pip\", \"install\", \"-e\", \".\"],\n",
    "                [\"pip\", \"install\", \"--upgrade\", \"flash-attn\"],\n",
    "                [\"wget\", f\"https://github.com/sunsmarterjie/yolov12/releases/download/v1.0/{yolo_model}\"]\n",
    "            ]\n",
    "\n",
    "            os.chdir(path=yolo_path)\n",
    "            for command in commands:\n",
    "                try:\n",
    "                    logging.info(msg=f\"Executing {' '.join(command)}\")\n",
    "                    subprocess.run(command, check=True)\n",
    "                    logging.info(msg=\"Command executed successfully\")\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    logging.info(msg=f\"Error while executing {command}. Details: {e}\")\n",
    "\n",
    "        device: Literal['cuda', 'cpu'] = \"cuda\" if is_available() else \"cpu\"\n",
    "        if device == 'cuda':\n",
    "            logging.info(msg=\"GPU detected. Using it.\")\n",
    "        elif device == 'cpu':\n",
    "            logging.info(msg=\"GPU not detected. Swichting to CPU.\")\n",
    "        \n",
    "        self.code_model = None\n",
    "        self.diagram_model = None\n",
    "        self.min_code_prediction_threshold = min_code_prediction_threshold\n",
    "        self.min_diagram_prediction_threshold = min_diagram_prediction_threshold\n",
    "        self.valid_subjects = {\"code\", \"diagram\"}\n",
    "        self.valid_images_extensions = {\"jpg\", \"jpeg\", \"png\", \"webp\"}\n",
    "    \n",
    "    def clear_cache(self) -> None:\n",
    "        empty_cache()\n",
    "        logging.info(msg=\"GPU cache has been cleared.\")\n",
    "\n",
    "    def is_subject_valid(self, subject: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the given subject extension is valid.\n",
    "\n",
    "        Args:\n",
    "            subject (str): The subject (e.g., \".code\").\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the subject is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        return subject.lower() in self.valid_subjects\n",
    "\n",
    "    def is_image_extension_valid(self, image_extension: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the given image extension is valid.\n",
    "\n",
    "        Args:\n",
    "            image_extension (str): The image extension (e.g., \".png\").\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the extension is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        return image_extension.lower() in self.valid_images_extensions\n",
    "    \n",
    "    def format_detections_to_predictions(self, detections) -> dict[str, list[Any]]:\n",
    "        formatted_predictions: list[Any] = []\n",
    "\n",
    "        for i, (xyxy, conf, cls) in enumerate(zip(detections.xyxy, detections.confidence, detections.class_id)):\n",
    "            x_min, y_min, x_max, y_max = xyxy\n",
    "            \n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            x_center = x_min + width / 2\n",
    "            y_center = y_min + height / 2\n",
    "\n",
    "            detection_id: str = str(uuid.uuid4())\n",
    "\n",
    "            formatted_predictions.append({\n",
    "                \"x\": round(number=float(x_center),ndigits=2),\n",
    "                \"y\": round(number=float(y_center), ndigits=2),\n",
    "                \"width\": round(number=float(width), ndigits=2),\n",
    "                \"height\": round(number=float(height), ndigits=2),\n",
    "                \"confidence\": round(number=float(conf),ndigits=3),\n",
    "                \"class\": \"code_snippet\",\n",
    "                \"class_id\": int(cls),\n",
    "                \"detection_id\": detection_id\n",
    "            })\n",
    "\n",
    "        return {\"predictions\": formatted_predictions}\n",
    "\n",
    "    def filter_images_with_code(self, images_path: Path) -> list[Path]:\n",
    "        if self.code_model is None:\n",
    "            raise ValueError(\"Code Model is not set. Run train_model(subject='code') or set_code_model(path='path/to/model') first.\")\n",
    "\n",
    "        valid_code_images_path: list[Path] = []\n",
    "\n",
    "        all_images_paths: list[Path] = [file for file in images_path.iterdir() if self.is_image_extension_valid(image_extension=file.suffix)]\n",
    "\n",
    "        for image_path in all_images_paths:\n",
    "            image = cv2.imread(str(image_path))\n",
    "            if image is None:\n",
    "                logging.error(msg=f\"Image: '{image_path.name}' could not be loaded. Skipping to the next image.\")\n",
    "                continue\n",
    "                \n",
    "            raw_code_detection_results = self.code_model(source=image, verbose=False)[0]\n",
    "            code_detection_resutls: Detections = Detections.from_ultralytics(ultralytics_results=raw_code_detection_results).with_nms()\n",
    "            code_predictions: dict[str, list[Any]] = self.format_detections_to_predictions(detections=code_detection_resutls)\n",
    "\n",
    "            if len(code_predictions[\"predictions\"]) > self.min_code_prediction_threshold:\n",
    "                valid_code_images_path.append(image_path)\n",
    "\n",
    "        return valid_code_images_path\n",
    "\n",
    "    def filter_iamges_with_diagrams(self, images_path: Path) -> list[Path]:\n",
    "        if self.diagram_model is None:\n",
    "            raise ValueError(\"Diagram Model is not set. Run train_model(subject='code') or set_diagram_model(path='path/to/model') first.\")\n",
    "\n",
    "        valid_diagram_images_path: list[Path] = []\n",
    "\n",
    "        all_images_paths: list[Path] = [file for file in images_path.iterdir() if self.is_image_extension_valid(image_extension=file.suffix)]\n",
    "\n",
    "        for image_path in all_images_paths:\n",
    "            image = cv2.imread(str(image_path))\n",
    "            if image is None:\n",
    "                logging.error(f\"Image: '{image_path.name}' could not be loaded. Skipping to the next image.\")\n",
    "                continue\n",
    "                \n",
    "            raw_diagram_detection_results = self.diagram_model(source=image, verbose=False)[0]\n",
    "            diagram_detection_resutls: Detections  = Detections.from_ultralytics(ultralytics_results=raw_diagram_detection_results).with_nms()\n",
    "            diagram_predictions: dict[str, list[Any]] = self.format_detections_to_predictions(detections=diagram_detection_resutls)\n",
    "\n",
    "            if len(diagram_predictions[\"predictions\"]) > self.min_diagram_prediction_threshold:\n",
    "                valid_diagram_images_path.append(image_path)\n",
    "\n",
    "        return valid_diagram_images_path    \n",
    "    \n",
    "    def filter_images_with(self, subject: str, images_path: Path) -> None:\n",
    "        if not self.is_subject_valid(subject):\n",
    "            logging.error(msg=f\"Subject is not valid. Only 'code' and 'diagram' are valid.\")\n",
    "            raise ValueError(\"Subject is not valid. Only 'code' and 'diagram' are valid.\")\n",
    "\n",
    "        match subject:\n",
    "            case \"code\":\n",
    "                self.filter_images_with_code(images_path)\n",
    "            case \"diagram\":\n",
    "                self.filter_iamges_with_diagrams(images_path)\n",
    "\n",
    "    def train_code_model_with_dataset(self, roboflowDataset: RoboflowDataset) -> None:\n",
    "        roboflowDataset.download_dataset()\n",
    "        roboflowDataset.find_dataset_path()\n",
    "        if(roboflowDataset.dataset_path is None):\n",
    "            logging.error(msg=\"Dataset path is not set. Run find_dataset_path() or check your params.\")\n",
    "            raise ValueError(\"Dataset path is not set. Run find_dataset_path() or check your params.\")\n",
    "\n",
    "        roboflowDataset.prepare_dataset_datayml()\n",
    "\n",
    "        model: Model = Model(model=self.code_model, data=roboflowDataset.dataset_path, epochs=300, device='cuda', batch=14, imgsz=640, optimizer=\"AdamW\")\n",
    "        model.train()\n",
    "\n",
    "    # def train_model_with_and_dataset(self, subject: str) -> None:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ROBOFLOW_API_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m MODELS_DIR: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m DATASET_DIR: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/datasets\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m API_KEY: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mROBOFLOW_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ROBOFLOW_API_KEY'"
     ]
    }
   ],
   "source": [
    "MODELS_DIR: str = '/teamspace/studios/this_studio/models'\n",
    "DATASET_DIR: str = '/teamspace/studios/this_studio/datasets'\n",
    "API_KEY: str = os.environ[\"ROBOFLOW_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DirectoryUtil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m models_dir: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mDirectoryUtil\u001b[49m\u001b[38;5;241m.\u001b[39mensure_directory(path\u001b[38;5;241m=\u001b[39mMODELS_DIR)\n\u001b[1;32m      3\u001b[0m yolov12: Yolo \u001b[38;5;241m=\u001b[39m Yolo(models_dir\u001b[38;5;241m=\u001b[39mmodels_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DirectoryUtil' is not defined"
     ]
    }
   ],
   "source": [
    "models_dir: str = DirectoryUtil.ensure_directory(path=MODELS_DIR)\n",
    "\n",
    "yolov12: Yolo = Yolo(models_dir=models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 17:26:11,786 - INFO - Directory already exists: /teamspace/studios/this_studio/datasets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 17:26:13,245 - INFO - Dataset is: /teamspace/studios/this_studio/datasets/Code-Snippet-Detection-22\n",
      "2025-03-20 17:26:13,246 - INFO - Dataset is ready to use for training.\n"
     ]
    }
   ],
   "source": [
    "datasets_path = DirectoryUtil.ensure_directory(path=DATASET_DIR)\n",
    "api_key: str = API_KEY\n",
    "the_workspace: str = 'bot-interactivo-tesis'\n",
    "project_id: str = 'code-snippet-detection'\n",
    "version_number: int = 22\n",
    "model_format: str = 'yolov12'\n",
    "\n",
    "codeRoboflowDataset: RoboflowDataset = RoboflowDataset(datasets_path=datasets_path,\n",
    "                                                        api_key=api_key,\n",
    "                                                        the_workspace=the_workspace,\n",
    "                                                        project_id=project_id,\n",
    "                                                        version_number=version_number,\n",
    "                                                        model_format=model_format)\n",
    "\n",
    "yolov12.train_code_model_with_dataset(roboflowDataset=codeRoboflowDataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
